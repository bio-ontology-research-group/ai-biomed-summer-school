{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c0f70b2",
   "metadata": {},
   "source": [
    "# mOWL Tutorial\n",
    "\n",
    "This tutorial will teach you how to use machine learning with ontologies. The tutorial captures the different approaches for generating OWL ontology embeddings, and methods to use them. We rely on the mOWL library which intends to implement all embedding methods for Semantic Web (OWL) ontologies.\n",
    "\n",
    "The majority of libraries for processing OWL ontologies are written in Java while most machine learning libraries are written in Python. First, we will need to access Java libraries in Python so that we can process ontologies and perform reasoning. We rely on the JPype library for this purpose which makes Java classes available in Python. We also have to set the memory available to the Java Virtual Machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93421ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mowl\n",
    "import tempfile\n",
    "mowl.init_jvm(\"40g\") # the amount of memory to assign to the JVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600f4b04",
   "metadata": {},
   "source": [
    "We can now access classes from the OWLAPI (the main reference implementation for processing Semantic Web ontologies) through their Python interfaces, just as we would in Java. The next code will load an ontology and classify it using the Elk reasoner. We then query for all subclasses of the Human Phenotype Ontology (HPO) class \"Mode of inheritance\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b658594",
   "metadata": {},
   "outputs": [],
   "source": [
    "from org.semanticweb.elk.owlapi import ElkReasonerFactory\n",
    "from org.semanticweb.owlapi.apibinding import OWLManager\n",
    "from org.semanticweb.owlapi.model import IRI\n",
    "\n",
    "manager = OWLManager.createOWLOntologyManager()\n",
    "fac = manager.getOWLDataFactory()\n",
    "ont = manager.loadOntologyFromOntologyDocument(IRI.create(\"file:merged-phenomenet.owl\"))\n",
    "print(\"Number of classes: \", ont.getClassesInSignature(True).size())\n",
    "\n",
    "reasoner_factory = ElkReasonerFactory()\n",
    "reasoner = reasoner_factory.createReasoner(ont)\n",
    "\n",
    "for i in reasoner.getSubClasses(fac.getOWLClass(IRI.create(\"http://purl.obolibrary.org/obo/HP_0000005\")), False).getFlattened():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3300340",
   "metadata": {},
   "source": [
    "mOWL wraps some functionality that is commonly used for generating ontology embeddings in the MOWLReasoner class, which can be used to compute a limited form of the deductive closure of an ontology.\n",
    "\n",
    "# Embedding ontologies\n",
    "\n",
    "mOWL implements several different ontology embeddings. The overall recipe of embedding ontologies is:\n",
    "* generate a Dataset for the ontology\n",
    "* project the OWL ontology suitable for an embedding\n",
    "* apply the embedding model\n",
    "* infer axioms using an inference model\n",
    "* (optional) evaluate the embeddings using an evaluation set\n",
    "\n",
    "## Datasets\n",
    "\n",
    "mOWL operates on OWL axioms, and every dataset consists of a set of OWL axioms (here, also called an ontology). mOWL also provides several datasets for testing purposes, and we will use a small dataset here first, the PPI Yeast Slim Dataset.\n",
    "\n",
    "PPIYeastSlimDataset consists of axioms from the Gene Ontology (GO), in particular the \"yeast slim\" of the GO, a set of yeast proteins, and an association between proteins and GO classes. The GO is natively available in OWL, but the associations are commonly available only as \"annotation\" file from various websites. This dataset makes a particular ontological commitment and represents all proteins as OWL classes. Given a protein $P$ and GO class $G$ that is an annotation of $P$, the following axiom is in the PPIYeastSlimDataset: $P \\sqsubseteq \\exists hasFunction.G$. PPIYeastSlimDataset further adds protein--protein interactions to the ontology; if protein $P_1$ interacts with $P_2$, the axioms $P_1 \\sqsubseteq \\exists interactsWith.P2$ and $P_2 \\sqsubseteq \\exists interactsWith.P_1$ are added.\n",
    "\n",
    "We can print the axioms in the ontology underlying `PPIYeastSlimDataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a867864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mowl.datasets.ppi_yeast import PPIYeastSlimDataset\n",
    "\n",
    "dataset = PPIYeastSlimDataset()\n",
    "dataset.get_labels()\n",
    "count = 0\n",
    "for i in dataset.ontology.getAxioms(True):\n",
    "    if count < 100:\n",
    "        print(i)\n",
    "        count += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c620d2",
   "metadata": {},
   "source": [
    "A Dataset may additionally have validation and testing data. Both validation and testing are again sets of axioms (ontologies). For the `PPIYeastSlimDataset`, both validation and testing is done only on interactions. We can investigate the axioms used for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a95d913",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in dataset.testing.getAxioms(True):\n",
    "    if count < 100:\n",
    "        print(i)\n",
    "        count += 1\n",
    "print(dataset.testing.getAxioms(True).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441ae52e",
   "metadata": {},
   "source": [
    "## Graph generation\n",
    "\n",
    "We generate a graph by projecting axioms in the ontology onto edges in a heterogeneous graph (or knowledge graph). There are several methods available for this operation, and we rely on the DL2Vec methods here which generates edges from axioms based on a set of patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9e8554",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mowl.projection.dl2vec.model import DL2VecProjector\n",
    "\n",
    "projector = DL2VecProjector( True)\n",
    "edges = projector.project(dataset.ontology)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc7f921",
   "metadata": {},
   "source": [
    "We can now visualize the graph generated using the `networkx` package (it's a bit large, so we just visualize parts of the graph):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a79676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pylab as plt\n",
    "\n",
    "elist = []\n",
    "count = 1000\n",
    "for i in edges:\n",
    "    if count > 0:\n",
    "        elist.append( (i.src(), i.dst()) )\n",
    "    count -= 1\n",
    "    \n",
    "G=nx.from_edgelist(elist)\n",
    "nx.draw(G, node_size=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa87459",
   "metadata": {},
   "source": [
    "Now that we generated a graph from the OWL axioms, we can embed the graph using any (heterogeneous) graph embedding method. The reason we need a method to embed \"heterogeneous\" graphs is that the projection operations we use consider the relation types, and they should be treated differently in the embedding. Fortunately, there are *many* methods to generate [Knowledge Graph Embeddings](https://persagen.com/files/misc/Wang2017Knowledge.pdf) and mOWL provides access to most of them either by directly implementing them or through the [PyKEEN library](https://github.com/pykeen/pykeen).\n",
    "\n",
    "Let's start by using embeddings based on random walks over the graph followed by Word2Vec. This method applies a repeated random walk starting from each node to generate a \"corpus\", followed by a word embedding that captures co-occurrence relations in this corpus. We have to set some parameters: the number of walks from each node; the length/depth of the random walk; a restart probability; and a file to write these walks to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262e7663",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mowl.walking.deepwalk.model import DeepWalk\n",
    "from mowl.walking.node2vec.model import Node2Vec\n",
    "\n",
    "tmp = tempfile.NamedTemporaryFile()\n",
    "walker = Node2Vec(\n",
    "\t              100, # number of walks\n",
    "\t\t\t\t  10, # length of each walk\n",
    "\t\t\t\t  0, # probability of restart\n",
    "\t\t\t\t  workers = 8, # number of usable CPUs\n",
    "                  outfile = tmp.name,\n",
    "                  q = 0.9\n",
    "\t\t\t\t  )\n",
    "\n",
    "walks = walker.walk(edges)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528dbbe6",
   "metadata": {},
   "source": [
    "We can now embed the corpus using a language model like Word2Vec. Word2Vec captures co-occurrence relations within a window. We just use a standard Word2Vec implementation here. Parameters we have to set is the embedding method (Skipgram or Continuous Bag Of Words), the minimum occurrence count of a word (should be set to `1` as otherwise some embeddings may be missing), the embedding size, the window size (within which co-occurrence is evaluated), and the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecedcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "corpus = LineSentence(tmp.name)\n",
    "w2v_model = Word2Vec(\n",
    "       corpus,\n",
    "       sg=1,\n",
    "       min_count=1,\n",
    "       vector_size=50,\n",
    "       window = 10,\n",
    "       epochs = 2,\n",
    "       workers = 16)\n",
    "\n",
    "vectors = w2v_model.wv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d5e15b",
   "metadata": {},
   "source": [
    "The result of the embedding is a set of vectors representing each word in the corpus, and therefore one vector for each entity that was included in the graph generated from the ontology axioms. We can visualize these embeddings using a TNSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaa2775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The mOWL TSNR is a wrapper for the sklearn.manifold tsne.\n",
    "from mowl.visualization.base import TSNE\n",
    "\n",
    "labels = dataset.get_labels() #PPIYeast or PPIYeastSlim datasets now contain the EC number labels. The labels are a dicttionary of the form entity_name -> label_name\n",
    "\n",
    "tsne = TSNE(vectors, labels)\n",
    "tsne.generate_points(5000, workers = 14, verbose = 1)\n",
    "tsne.show()\n",
    "\n",
    "#TSNE plot can be saved using the following line:\n",
    "#tsne.savefig(path_to_image.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e9a225",
   "metadata": {},
   "source": [
    "As we can see, the embeddings (somehow) cluster according to their Enzyme Classification. We may then use a similarity function between the embedding vectors to generate \"meaningful\" relations. In mOWL, meaningful relations between OWL entities are expressed in the form of OWL axioms. To obtain axioms from the embeddings, we need an inference method that uses similarity to determine axioms. Here, we rely on cosine similarity between (proteins) $X$ and $Y$ in order to predict axioms of the form $X \\sqsubseteq \\exists interactsWith.Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aee03a6",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Visualizing the embeddings is nice, but we may want some more specific answers from these embeddings. In particular, we can use them to infer OWL axioms that should hold true. For performing inference, we need two ingredients. First, we need an axiom (type) that we would like to infer; here we want to infer axioms of the type \"X SubClassOf: interacts-with some Y\" (because these are the axioms in our test set). The way mOWL implements the inference is that it computes a score for query axioms, possibly iterating through sets of classes; the scoring function can be chosen depending on the type of embedding that is used; here, we just use cosine similarity between the embeddings of the class \"X\" and \"Y\" to compute the score of \"X SubClassOf: interacts-with some Y\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d005d692",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mowl.inference.cosine import CosineSimilarityInfer\n",
    "\n",
    "cosine_infer = CosineSimilarityInfer(vectors, \"http://interacts_with\")\n",
    "preds = cosine_infer.score(\"c?.*?4932\\.(Q).*? SubClassOf http://interacts_with some  c?.*?4932.*?\")\n",
    "len(preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e238550",
   "metadata": {},
   "source": [
    "Now we can look at some of the scores of these axioms; based on the scores, we can also compute metrics with respect to a test set of axioms, including recall at certain ranks, or ROCAUC, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a116e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_it = iter(preds.items())\n",
    "for i in range(10):\n",
    "    print(next(preds_it))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b23b3c",
   "metadata": {},
   "source": [
    "## Translating embeddings\n",
    "\n",
    "Random walks are a form of embedding of graphs that relies on adjacency. However, other knowledge graph embeddings are more explicit about the kind of graph properties they preserve. For example, [TransE](https://paperswithcode.com/method/transe) generates embeddings $e$ for nodes and edge types such that $e(h) + e(r) \\approx e(t)$ if $r(h,t)$ is an edge in the graph. But there are hundreds of similar embedding methods available, and we rely on the PyKEEN library for accessing these kinds of embeddings. Here is an example of using TransE to generate an embedding of the projected graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db2d2fd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from mowl.embeddings.translational.model import TranslationalOnt\n",
    "import torch\n",
    "cuda0 = torch.device('cuda:0')\n",
    "\n",
    "\n",
    "trans_model = TranslationalOnt(\n",
    "     edges,\n",
    "     trans_method = \"transE\",\n",
    "     embedding_dim = 50,\n",
    "     epochs = 10,\n",
    "     batch_size = 1024,\n",
    "#     device = cuda0,\n",
    "     model_filepath = \"/tmp/trans_model.th\"\n",
    " )\n",
    "\n",
    "trans_model.train()\n",
    "cls_embeddings, rel_embeddings = trans_model.get_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c0fa01",
   "metadata": {},
   "source": [
    "We can now visualize these embeddings as before, coloring the vectors by their EC number (useful to see how well the embeddings work for a classification task):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b638678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mowl.visualization.base import TSNE\n",
    "\n",
    "labels = dataset.get_labels() #PPIYeast or PPIYeastSlim datasets now contain the EC number labels. The labels are a dicttionary of the form entity_name -> label_name\n",
    "tsne = TSNE(cls_embeddings, labels)\n",
    "tsne.generate_points(5000, workers = 16, verbose = 1)\n",
    "tsne.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd4f6b2",
   "metadata": {},
   "source": [
    "Similar as before, we can apply a function to score axioms. Here, we use a different function that relies on how the TransE model performs link prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c34ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mowl.inference.el import GCI2Score\n",
    "gci2_scorer = GCI2Score(trans_model.score_method_point, list(cls_embeddings.keys()), list(rel_embeddings.keys())) #in this case we need to imput the class and relations list.\n",
    "print(f\"Accepted pattern: {gci2_scorer.patterns}\")\n",
    "\n",
    "preds = gci2_scorer.score(\"c?.*?4932.Q0110.*? SubClassOf p?.*?int.*? some c?.*?4932.Q0.*?\")\n",
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8625008e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_it = iter(preds.items())\n",
    "for i in range(10):\n",
    "    print(next(preds_it))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec980310",
   "metadata": {},
   "source": [
    "## Embeddings without graph projection\n",
    "\n",
    "So far, we have just reused knowledge graph embedding methods, and first projected ontologies onto graphs. This works great, in particular as there are so many knowledge graph embedding methods available. However, there are some disadvantages; in particular, projecting axioms onto graphs will almost always lose some information. For example, almost no graph projection method will adequately deal with disjointness between classes, or they may not consider axioms of a certain complexity. mOWL implements a number of embedding methods that are based directly on axioms. We can try one of the simplest methods, Onto2Vec, which simply applies a language model to the ontology axioms directly. We first extract the axioms as a \"corpus\" (set of sentences), and then we embed this using Word2Vec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb2d589",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mowl.corpus.base import extract_and_save_axiom_corpus, extract_annotation_corpus\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "tmp = tempfile.NamedTemporaryFile()\n",
    "extract_and_save_axiom_corpus(dataset.ontology, tmp.name)\n",
    "extract_annotation_corpus(dataset.ontology, tmp.name)\n",
    "\n",
    "sentences = LineSentence(tmp.name)\n",
    "\n",
    "model = Word2Vec(\n",
    "         sentences,\n",
    "         sg = 1,\n",
    "         min_count = 1,\n",
    "         vector_size = 20,\n",
    "         window = 5,\n",
    "         epochs = 20,\n",
    "         workers = 8\n",
    "     )\n",
    "\n",
    "vectors = model.wv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb87d798",
   "metadata": {},
   "source": [
    "As before we can now visualize the embeddings generated:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406d39a8",
   "metadata": {},
   "source": [
    "from mowl.visualization.base import TSNE\n",
    "\n",
    "labels = dataset.get_labels() #PPIYeast or PPIYeastSlim datasets now contain the EC number labels. The labels are a dicttionary of the form entity_name -> label_name\n",
    "tsne = TSNE(vectors, labels)\n",
    "tsne.generate_points(5000, workers = 16, verbose = 1)\n",
    "tsne.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533b8152",
   "metadata": {},
   "source": [
    "## EL Embeddings\n",
    "\n",
    "The embedding methods so far did not really exploit any of the semantics of the OWL language. The last embedding model here is EL Embeddings; EL Embeddings generate a model *as* the embedding, i.e., with respect to a particular set of operations, the resulting embeddings generate (or approximate) a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1f8913",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mowl.embeddings.elembeddings.model import ELEmbeddings\n",
    "import torch\n",
    "\n",
    "cuda0 = torch.device('cpu')\n",
    "\n",
    "model = ELEmbeddings(\n",
    "     dataset,\n",
    "     epochs = 10,\n",
    "     margin = 0.1,\n",
    "     model_filepath = \"model.th\",\n",
    "    device = cuda0\n",
    " )\n",
    "\n",
    "model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70185682",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.get_evaluation_classes() # returns all classes in the axioms in testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5fc1a0",
   "metadata": {},
   "source": [
    "As before, we can perform inference on these classes, using the specific scoring function of EL Embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb24160",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mowl.inference.el import GCI2Score\n",
    "elem_cls_embs, elem_rel_embs = model.get_embeddings()\n",
    "gci2_score_elem = GCI2Score(model.gci2_loss, list(elem_cls_embs.keys()), list(elem_rel_embs.keys())) #in this case we need to imput the class and relations list.\n",
    "print(f\"Accepted pattern: {gci2_score_elem.patterns}\")\n",
    "\n",
    "preds_elem = gci2_score_elem.score(\"c?.*?4932.Q0110.*? SubClassOf p?.*?int.*? some c?.*?4932.Q0.*?\")\n",
    "len(preds_elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d938612",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_it = iter(preds_elem.items())\n",
    "for i in range(10):\n",
    "    print(next(preds_it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f81079",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
