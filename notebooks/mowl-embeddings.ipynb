{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c0f70b2",
   "metadata": {},
   "source": [
    "# mOWL Tutorial\n",
    "\n",
    "This tutorial will teach you how to use machine learning with ontologies. The tutorial captures the different approaches for generating OWL ontology embeddings, and methods to use them. We rely on the mOWL library which intends to implement all embedding methods for Semantic Web (OWL) ontologies.\n",
    "\n",
    "The majority of libraries for processing OWL ontologies are written in Java while most machine learning libraries are written in Python. First, we will need to access Java libraries in Python so that we can process ontologies and perform reasoning. We rely on the JPype library for this purpose which makes Java classes available in Python. We also have to set the memory available to the Java Virtual Machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93421ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mowl\n",
    "mowl.init_jvm(\"40g\") # the amount of memory to assign to the JVM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600f4b04",
   "metadata": {},
   "source": [
    "We can now access classes from the OWLAPI (the main reference implementation for processing Semantic Web ontologies) through their Python interfaces, just as we would in Java. The next code will load an ontology and classify it using the Elk reasoner. We then query for all subclasses of the Human Phenotype Ontology (HPO) class \"Mode of inheritance\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b658594",
   "metadata": {},
   "outputs": [],
   "source": [
    "from org.semanticweb.elk.owlapi import ElkReasonerFactory\n",
    "from org.semanticweb.owlapi.apibinding import OWLManager\n",
    "from org.semanticweb.owlapi.model import IRI\n",
    "\n",
    "manager = OWLManager.createOWLOntologyManager()\n",
    "fac = manager.getOWLDataFactory()\n",
    "ont = manager.loadOntologyFromOntologyDocument(IRI.create(\"file:merged-phenomenet.owl\"))\n",
    "print(\"Number of classes: \", ont.getClassesInSignature(True).size())\n",
    "\n",
    "reasoner_factory = ElkReasonerFactory()\n",
    "reasoner = reasoner_factory.createReasoner(ont)\n",
    "\n",
    "for i in reasoner.getSubClasses(fac.getOWLClass(IRI.create(\"http://purl.obolibrary.org/obo/HP_0000005\")), False).getFlattened():\n",
    "    print(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3300340",
   "metadata": {},
   "source": [
    "mOWL wraps some functionality that is commonly used for generating ontology embeddings in the MOWLReasoner class, which can be used to compute a limited form of the deductive closure of an ontology.\n",
    "\n",
    "# Embedding ontologies\n",
    "\n",
    "mOWL implements several different ontology embeddings. The overall recipe of embedding ontologies is:\n",
    "* generate a Dataset for the ontology\n",
    "* project the OWL ontology suitable for an embedding\n",
    "* apply the embedding model\n",
    "* infer axioms using an inference model\n",
    "* (optional) evaluate the embeddings using an evaluation set\n",
    "\n",
    "## Datasets\n",
    "\n",
    "mOWL operates on OWL axioms, and every dataset consists of a set of OWL axioms (here, also called an ontology). mOWL also provides several datasets for testing purposes, and we will use a small dataset here first, the PPI Yeast Slim Dataset.\n",
    "\n",
    "PPIYeastSlimDataset consists of axioms from the Gene Ontology (GO), in particular the \"yeast slim\" of the GO, a set of yeast proteins, and an association between proteins and GO classes. The GO is natively available in OWL, but the associations are commonly available only as \"annotation\" file from various websites. This dataset makes a particular ontological commitment and represents all proteins as OWL classes. Given a protein $P$ and GO class $G$ that is an annotation of $P$, the following axiom is in the PPIYeastSlimDataset: $P \\sqsubseteq \\exists hasFunction.G$. PPIYeastSlimDataset further adds protein--protein interactions to the ontology; if protein $P_1$ interacts with $P_2$, the axioms $P_1 \\sqsubseteq \\exists interactsWith.P2$ and $P_2 \\sqsubseteq \\exists interactsWith.P_1$ are added.\n",
    "\n",
    "We can print the axioms in the ontology underlying `PPIYeastSlimDataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a867864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mowl.datasets.ppi_yeast import PPIYeastSlimDataset\n",
    "\n",
    "dataset = PPIYeastSlimDataset()\n",
    "for i in dataset.ontology.getAxioms(True):\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c620d2",
   "metadata": {},
   "source": [
    "A Dataset may additionally have validation and testing data. Both validation and testing are again sets of axioms (ontologies). For the `PPIYeastSlimDataset`, both validation and testing is done only on interactions. We can investigate the axioms used for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a95d913",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dataset.testing.getAxioms(True):\n",
    "    print(i)\n",
    "print(dataset.testing.getAxioms(True).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441ae52e",
   "metadata": {},
   "source": [
    "## Graph generation\n",
    "\n",
    "projection operation from axioms into graphs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bbaac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from org.mowl.Projectors import DL2VecProjector\n",
    "projector = DL2VecProjector( True)\n",
    "edges = projector.project(dataset.ontology)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a79676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pylab as plt\n",
    "\n",
    "elist = []\n",
    "count = 5000\n",
    "for i in edges:\n",
    "    if count > 0:\n",
    "        elist.append( (i.src(), i.dst()) )\n",
    "    count -= 1\n",
    "    \n",
    "G=nx.from_edgelist(elist)\n",
    "nx.draw(G, node_size=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741402c7",
   "metadata": {},
   "source": [
    "Now that we generated a graph from the OWL axioms, we can embed the graph using any (heterogeneous) graph embedding method. The reason we need a method to embed \"heterogeneous\" graphs is that the projection operations we use consider the relation types, and they should be treated differently in the embedding. Fortunately, there are *many* methods to generate [Knowledge Graph Embeddings](https://persagen.com/files/misc/Wang2017Knowledge.pdf) and mOWL provides access to most of them either by directly implementing them or through the [PyKEEN library](https://github.com/pykeen/pykeen).\n",
    "\n",
    "Let's start by using embeddings based on random walks over the graph followed by Word2Vec. This method applies a repeated random walk starting from each node to generate a \"corpus\", followed by a word embedding that captures co-occurrence relations in this corpus. We have to set some parameters: the number of walks from each node; the length/depth of the random walk; a restart probability; and a file to write these walks to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262e7663",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mowl.walking.deepwalk.model import DeepWalk\n",
    "walker = DeepWalk(\n",
    "\t              100, # number of walks\n",
    "\t\t\t\t  10, # length of each walk\n",
    "\t\t\t\t  0, # probability of restart\n",
    "\t\t\t\t  workers = 8, # number of usable CPUs\n",
    "                  outfile = '/tmp/a.out'\n",
    "\t\t\t\t  )\n",
    "\n",
    "walks = walker.walk(edges)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b42a60",
   "metadata": {},
   "source": [
    "We can now embed the corpus using a language model like Word2Vec. Word2Vec captures co-occurrence relations within a window. We just use a standard Word2Vec implementation here. Parameters we have to set is the embedding method (Skipgrap or Continuout Bag Of Words), the minimum occurrence count of a word (should be set to `1` as otherwise some embeddings may be missing), the embedding size, the window size (within which co-occurrence is evaluated), and the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecedcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "\n",
    "corpus = LineSentence(\"/tmp/a.out\")\n",
    "\n",
    "w2v_model = Word2Vec(\n",
    "       corpus,\n",
    "       sg=1,\n",
    "       min_count=1,\n",
    "       vector_size=3,\n",
    "       window = 10,\n",
    "       epochs = 10,\n",
    "       workers = 16)\n",
    "\n",
    "vectors = w2v_model.wv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2fa9bf",
   "metadata": {},
   "source": [
    "The result of the embedding is a set of vectors representing each word in the corpus, and therefore one vector for each entity that was included in the graph generated from the ontology axioms. We can visualize these embeddings using a TNSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba1000f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = [i for i in w2v_model.wv.key_to_index]\n",
    "tokens = w2v_model[labels]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07b2a6e",
   "metadata": {},
   "source": [
    "As we can see, the embeddings (somehow) cluster according to their Enzyme Classification. We may then use a similarity function between the embedding vectors to generate \"meaningful\" relations. In mOWL, meaningful relations between OWL entities are expressed in the form of OWL axioms. To obtain axioms from the embeddings, we need an inference method that uses similarity to determine axioms. Here, we rely on cosine similarity between (proteins) $X$ and $Y$ in order to predict axioms of the form $X \\sqsubseteq \\exists interactsWith.Y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee4259e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "765002f0",
   "metadata": {},
   "source": [
    "Random walks are a form of embedding of graphs that relies on adjacency. However, other knowledge graph embeddings are more explicit about the kind of graph properties they preserve. For example, [TransE](https://paperswithcode.com/method/transe) generates embeddings $e$ for nodes and edge types such that $e(h) + e(r) \\approx e(t)$ if $r(h,t)$ is an edge in the graph. But there are hundreds of similar embedding methods available, and we rely on the PyKEEN library for accessing these kinds of embeddings. Here is an example of using TransE to generate an embedding of the projected graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8db2d2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pykeen.models.base:No random seed is specified. This may lead to non-reproducible results.\n",
      "INFO:pykeen.triples.triples_factory:Creating inverse triples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "153fc4cbf2fd4e9fa994ab7f99f8be9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training epochs on cpu:   0%|          | 0/20 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pykeen.triples.triples_factory:Creating inverse triples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cpu:   0%|          | 0/15577 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cpu:   0%|          | 0/15577 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cpu:   0%|          | 0/15577 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cpu:   0%|          | 0/15577 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cpu:   0%|          | 0/15577 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cpu:   0%|          | 0/15577 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cpu:   0%|          | 0/15577 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cpu:   0%|          | 0/15577 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cpu:   0%|          | 0/15577 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cpu:   0%|          | 0/15577 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cpu:   0%|          | 0/15577 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cpu:   0%|          | 0/15577 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cpu:   0%|          | 0/15577 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cpu:   0%|          | 0/15577 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cpu:   0%|          | 0/15577 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cpu:   0%|          | 0/15577 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cpu:   0%|          | 0/15577 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cpu:   0%|          | 0/15577 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cpu:   0%|          | 0/15577 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training batches on cpu:   0%|          | 0/15577 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mowl.embeddings.translational.model import TranslationalOnt\n",
    "import torch\n",
    "cuda0 = torch.device('cuda:0')\n",
    "\n",
    "trans_model = TranslationalOnt(\n",
    "     edges,\n",
    "     trans_method = \"transE\",\n",
    "     embedding_dim = 50,\n",
    "     epochs = 20,\n",
    "     batch_size = 32\n",
    " ).to()\n",
    "\n",
    "trans_model.train()\n",
    "embeddings = trans_model.get_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c687d762",
   "metadata": {},
   "source": [
    "## Embeddings without projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1f8913",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mowl.embeddings.elembeddings.model import ELEmbeddings\n",
    "import torch\n",
    "\n",
    "cuda0 = torch.device('cuda:0')\n",
    "\n",
    "model = ELEmbeddings(\n",
    "     dataset,\n",
    "     epochs = 1000,\n",
    "     margin = 0.1,\n",
    "     model_filepath = \"model.th\",\n",
    "     device = cuda0\n",
    " )\n",
    "\n",
    "model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70185682",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.get_evaluation_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb24160",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
